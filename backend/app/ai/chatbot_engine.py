"""
Enhanced AI Chatbot Engine for Performance Analysis
Provides intelligent responses based on performance metrics
"""
from typing import List, Dict, Any
import json
import re

class PerformanceChatbot:
    """Advanced chatbot for analyzing performance test results"""
    
    # Comprehensive sample prompts organized by category
    SAMPLE_PROMPTS = {
        "overview": [
            "Give me an overview of the test results",
            "What's the overall performance grade?",
            "How did the system perform overall?",
            "Summarize the test results",
            "What's the executive summary?",
        ],
        "response_times": [
            "What are the response times?",
            "Show me the average response time",
            "What's the 95th percentile response time?",
            "Which endpoints are the slowest?",
            "What's the median response time?",
            "Show me all percentile data (70th, 80th, 90th, 95th, 99th)",
        ],
        "errors": [
            "What's the error rate?",
            "Show me all errors",
            "Which endpoints have the most errors?",
            "What types of errors occurred?",
            "How many requests failed?",
            "What's the success rate?",
        ],
        "performance_issues": [
            "What are the critical issues?",
            "What problems were identified?",
            "What's causing poor performance?",
            "Which endpoints need optimization?",
            "What are the bottlenecks?",
            "What's failing or slow?",
        ],
        "recommendations": [
            "How can I improve performance?",
            "What are your recommendations?",
            "How do I fix these issues?",
            "What should I optimize first?",
            "What's the action plan?",
            "How do I achieve A+ grade?",
        ],
        "comparisons": [
            "How does this compare to industry standards?",
            "Are we meeting SLA requirements?",
            "Compare current vs target metrics",
            "What metrics are below target?",
            "What metrics are passing?",
        ],
        "business_impact": [
            "What's the business impact?",
            "How does this affect users?",
            "What's the cost of these issues?",
            "What's the ROI of fixing this?",
            "How does performance affect revenue?",
        ],
        "specific_metrics": [
            "What's the throughput?",
            "Show me SLA compliance",
            "What's the availability?",
            "How many requests were tested?",
            "What was the test duration?",
            "Show me transaction statistics",
        ],
        "trends": [
            "Show me the worst performing transactions",
            "What are the top 10 slowest endpoints?",
            "Which requests have high error rates?",
            "Show me performance breakdown by endpoint",
        ],
    }
    
    @staticmethod
    def get_sample_prompts() -> Dict[str, List[str]]:
        """Get all sample prompts organized by category"""
        return PerformanceChatbot.SAMPLE_PROMPTS
    
    @staticmethod
    def get_random_prompts(count: int = 6) -> List[str]:
        """Get random sample prompts for quick access"""
        import random
        all_prompts = []
        for category_prompts in PerformanceChatbot.SAMPLE_PROMPTS.values():
            all_prompts.extend(category_prompts)
        return random.sample(all_prompts, min(count, len(all_prompts)))
    
    @staticmethod
    def analyze_query_intent(message: str) -> str:
        """Determine the intent of the user's query"""
        message_lower = message.lower()
        
        # Intent keywords mapping
        intent_keywords = {
            "overview": ["overview", "summary", "overall", "general", "executive", "report"],
            "grade": ["grade", "score", "rating", "assessment"],
            "response_time": ["response", "latency", "time", "speed", "slow", "fast", "percentile"],
            "error": ["error", "fail", "failure", "problem", "issue", "bug"],
            "throughput": ["throughput", "rps", "requests per second", "volume", "load"],
            "availability": ["availability", "uptime", "downtime", "success rate"],
            "sla": ["sla", "compliance", "meet", "target", "requirement"],
            "recommendation": ["recommend", "suggest", "improve", "fix", "optimize", "enhance", "better"],
            "critical": ["critical", "urgent", "important", "priority", "severe"],
            "comparison": ["compare", "vs", "versus", "difference", "benchmark", "standard"],
            "business": ["business", "impact", "revenue", "cost", "roi", "user"],
            "endpoint": ["endpoint", "api", "transaction", "request", "url"],
            "specific_metric": ["median", "average", "mean", "max", "min", "p95", "p99", "70th", "80th", "90th"],
        }
        
        # Check for intent
        for intent, keywords in intent_keywords.items():
            if any(keyword in message_lower for keyword in keywords):
                return intent
        
        return "general"
    
    @staticmethod
    def generate_response(message: str, context_data: List[Dict]) -> str:
        """
        Generate intelligent AI response based on message and context
        
        Args:
            message: User's question
            context_data: List of analyzed file data with metrics
            
        Returns:
            Comprehensive response string
        """
        if not context_data:
            return PerformanceChatbot._no_context_response()
        
        intent = PerformanceChatbot.analyze_query_intent(message)
        
        # Route to appropriate handler
        if intent == "overview":
            return PerformanceChatbot._generate_overview_response(context_data)
        elif intent == "grade":
            return PerformanceChatbot._generate_grade_response(context_data)
        elif intent == "response_time":
            return PerformanceChatbot._generate_response_time_analysis(context_data)
        elif intent == "error":
            return PerformanceChatbot._generate_error_analysis(context_data)
        elif intent == "throughput":
            return PerformanceChatbot._generate_throughput_analysis(context_data)
        elif intent == "availability":
            return PerformanceChatbot._generate_availability_analysis(context_data)
        elif intent == "sla":
            return PerformanceChatbot._generate_sla_analysis(context_data)
        elif intent == "recommendation":
            return PerformanceChatbot._generate_recommendations(context_data)
        elif intent == "critical":
            return PerformanceChatbot._generate_critical_issues(context_data)
        elif intent == "comparison":
            return PerformanceChatbot._generate_comparison(context_data)
        elif intent == "business":
            return PerformanceChatbot._generate_business_impact(context_data)
        elif intent == "endpoint":
            return PerformanceChatbot._generate_endpoint_analysis(context_data)
        elif intent == "specific_metric":
            return PerformanceChatbot._generate_specific_metrics(context_data, message)
        else:
            return PerformanceChatbot._generate_general_response(context_data, message)
    
    @staticmethod
    def _no_context_response() -> str:
        """Response when no analyzed files are available"""
        return """ü§ñ **No Analysis Data Available**

I need analyzed files to provide insights. Here's how to get started:

1Ô∏è‚É£ **Upload Files**: Go to Upload page and upload your JMeter/performance test files
2Ô∏è‚É£ **Analyze**: Go to Analysis page and analyze the uploaded files
3Ô∏è‚É£ **Ask Questions**: Come back and ask me anything about your results!

**Example questions you can ask:**
‚Ä¢ What's the overall performance grade?
‚Ä¢ Show me the error rates
‚Ä¢ What are the response times?
‚Ä¢ Give me recommendations for improvement
‚Ä¢ What are the critical issues?

I'm here to help you understand your performance test results! üöÄ"""
    
    @staticmethod
    def _generate_overview_response(context_data: List[Dict]) -> str:
        """Generate comprehensive overview"""
        responses = ["üìä **Performance Test Results Overview**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            category = data.get("category", "unknown")
            metrics = data.get("metrics", {})
            
            if category == "jmeter":
                summary = metrics.get("summary", {})
                grade = summary.get("overall_grade", "N/A")
                score = summary.get("overall_score", 0)
                success_rate = summary.get("success_rate", 0)
                
                sample_time = metrics.get("sample_time", {})
                avg_time = sample_time.get("mean", 0) / 1000
                p95 = sample_time.get("p95", 0) / 1000
                
                error_rate = metrics.get("error_rate", 0) * 100
                throughput = metrics.get("throughput", 0)
                total_samples = metrics.get("total_samples", 0)
                
                responses.append(f"""
**üìÅ {filename}**

üéØ **Overall Grade:** {grade} ({score:.0f}/100)
‚úÖ **Success Rate:** {success_rate:.1f}%
‚ö° **Avg Response:** {avg_time:.2f}s
üìà **95th Percentile:** {p95:.2f}s
‚ùå **Error Rate:** {error_rate:.2f}%
üîÑ **Throughput:** {throughput:.1f} req/s
üìä **Total Requests:** {total_samples:,}

**Grade Breakdown:**
""")
                
                grade_reasons = summary.get("grade_reasons", {})
                for category_name, display_name in [
                    ("performance", "‚ö° Performance"),
                    ("reliability", "üõ°Ô∏è Reliability"),
                    ("user_experience", "üë• User Experience"),
                    ("scalability", "üìà Scalability")
                ]:
                    gr = grade_reasons.get(category_name, {})
                    responses.append(f"‚Ä¢ {display_name}: **{gr.get('grade', 'N/A')}** - {gr.get('reason', 'N/A')}")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_grade_response(context_data: List[Dict]) -> str:
        """Generate detailed grade analysis"""
        responses = ["üéØ **Performance Grade Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            
            grade = summary.get("overall_grade", "N/A")
            score = summary.get("overall_score", 0)
            scores = summary.get("scores", {})
            
            responses.append(f"""
**üìÅ {filename}**

**Overall Grade: {grade}** ({score:.0f}/100)

**Detailed Scores:**
‚Ä¢ ‚ö° Performance: {scores.get('performance', 0):.0f}/100
‚Ä¢ üõ°Ô∏è Reliability: {scores.get('reliability', 0):.0f}/100
‚Ä¢ üë• User Experience: {scores.get('user_experience', 0):.0f}/100
‚Ä¢ üìà Scalability: {scores.get('scalability', 0):.0f}/100

**Score Breakdown:**
‚Ä¢ Availability: {scores.get('availability', 0):.0f}/100
‚Ä¢ Response Time: {scores.get('response_time', 0):.0f}/100
‚Ä¢ Error Rate: {scores.get('error_rate', 0):.0f}/100
‚Ä¢ Throughput: {scores.get('throughput', 0):.0f}/100
‚Ä¢ 95th Percentile: {scores.get('p95_percentile', 0):.0f}/100
‚Ä¢ SLA Compliance: {scores.get('sla_compliance', 0):.0f}/100

**Grading Scale:**
‚Ä¢ A+ (90-100): Exceptional
‚Ä¢ A (80-89): Excellent
‚Ä¢ B+ (75-79): Good
‚Ä¢ B (70-74): Acceptable
‚Ä¢ C+ (65-69): Marginal ‚Üê Current: {grade}
‚Ä¢ D (50-59): Critical
""")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_response_time_analysis(context_data: List[Dict]) -> str:
        """Generate response time analysis"""
        responses = ["‚ö° **Response Time Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            sample_time = metrics.get("sample_time", {})
            
            responses.append(f"""
**üìÅ {filename}**

**Response Time Statistics:**
‚Ä¢ Minimum: {sample_time.get('min', 0)/1000:.2f}s
‚Ä¢ Median: {sample_time.get('median', 0)/1000:.2f}s
‚Ä¢ Average: {sample_time.get('mean', 0)/1000:.2f}s
‚Ä¢ 70th Percentile: {sample_time.get('p70', 0)/1000:.2f}s
‚Ä¢ 80th Percentile: {sample_time.get('p80', 0)/1000:.2f}s
‚Ä¢ 90th Percentile: {sample_time.get('p90', 0)/1000:.2f}s
‚Ä¢ 95th Percentile: {sample_time.get('p95', 0)/1000:.2f}s
‚Ä¢ 99th Percentile: {sample_time.get('p99', 0)/1000:.2f}s
‚Ä¢ Maximum: {sample_time.get('max', 0)/1000:.2f}s

**Analysis:**
""")
            
            avg = sample_time.get('mean', 0) / 1000
            p95 = sample_time.get('p95', 0) / 1000
            
            if avg < 1:
                responses.append("‚úÖ Excellent average response time!")
            elif avg < 2:
                responses.append("‚úÖ Good average response time, meets SLA")
            elif avg < 5:
                responses.append("‚ö†Ô∏è Average response time needs improvement")
            else:
                responses.append("‚ùå Critical: Response time is too slow")
            
            if p95 < 3:
                responses.append("‚úÖ 95th percentile is within acceptable range")
            else:
                responses.append("‚ö†Ô∏è 95th percentile indicates some slow requests")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_error_analysis(context_data: List[Dict]) -> str:
        """Generate error rate analysis"""
        responses = ["‚ùå **Error Rate Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            
            error_rate = metrics.get("error_rate", 0) * 100
            total_errors = metrics.get("total_errors", 0)
            total_samples = metrics.get("total_samples", 0)
            response_codes = metrics.get("response_codes", {})
            
            responses.append(f"""
**üìÅ {filename}**

**Error Statistics:**
‚Ä¢ Total Errors: {total_errors:,}
‚Ä¢ Total Requests: {total_samples:,}
‚Ä¢ Error Rate: {error_rate:.2f}%
‚Ä¢ Success Rate: {100-error_rate:.2f}%

**Response Code Distribution:**
""")
            
            for code, count in sorted(response_codes.items(), key=lambda x: x[1], reverse=True)[:10]:
                percentage = (count / total_samples * 100) if total_samples > 0 else 0
                status = "‚úÖ" if code.startswith("2") else "‚ö†Ô∏è" if code.startswith("4") else "‚ùå"
                responses.append(f"{status} {code}: {count:,} ({percentage:.2f}%)")
            
            responses.append("\n**Analysis:**")
            if error_rate < 0.5:
                responses.append("‚úÖ Excellent error rate!")
            elif error_rate < 1:
                responses.append("‚úÖ Good error rate, within acceptable limits")
            elif error_rate < 3:
                responses.append("‚ö†Ô∏è Error rate needs attention")
            else:
                responses.append("‚ùå Critical: High error rate affecting reliability")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_recommendations(context_data: List[Dict]) -> str:
        """Generate recommendations"""
        responses = ["üí° **Performance Improvement Recommendations**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            recommendations = summary.get("recommendations", [])
            
            responses.append(f"**üìÅ {filename}**\n")
            
            if recommendations:
                for rec in recommendations:
                    category = rec.get("category", "General")
                    priority = rec.get("priority", "Medium")
                    items = rec.get("items", [])
                    
                    responses.append(f"\n**{category}** (Priority: {priority})")
                    for item in items[:5]:
                        responses.append(f"‚Ä¢ {item}")
            else:
                responses.append("‚úÖ No specific recommendations - system performing well!")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_critical_issues(context_data: List[Dict]) -> str:
        """Generate critical issues analysis"""
        responses = ["üî¥ **Critical Issues Identified**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            critical_issues = summary.get("critical_issues", [])
            
            responses.append(f"**üìÅ {filename}**\n")
            
            if critical_issues:
                for i, issue in enumerate(critical_issues, 1):
                    responses.append(f"""
**Issue #{i}: {issue.get('title', 'Unknown')}**
‚Ä¢ **Impact:** {issue.get('impact', 'N/A')}
‚Ä¢ **Affected:** {issue.get('affected', 'N/A')}
‚Ä¢ **Priority:** {issue.get('priority', 'N/A')}
‚Ä¢ **Timeline:** {issue.get('timeline', 'N/A')}
""")
            else:
                responses.append("‚úÖ No critical issues identified!")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_sla_analysis(context_data: List[Dict]) -> str:
        """Generate SLA compliance analysis"""
        responses = ["üìä **SLA Compliance Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            
            sla_2s = summary.get("sla_compliance_2s", 0)
            sla_3s = summary.get("sla_compliance_3s", 0)
            sla_5s = summary.get("sla_compliance_5s", 0)
            
            responses.append(f"""
**üìÅ {filename}**

**SLA Compliance:**
‚Ä¢ ‚â§ 2 seconds: {sla_2s:.1f}% {'‚úÖ' if sla_2s >= 95 else '‚ö†Ô∏è' if sla_2s >= 80 else '‚ùå'}
‚Ä¢ ‚â§ 3 seconds: {sla_3s:.1f}% {'‚úÖ' if sla_3s >= 95 else '‚ö†Ô∏è' if sla_3s >= 80 else '‚ùå'}
‚Ä¢ ‚â§ 5 seconds: {sla_5s:.1f}% {'‚úÖ' if sla_5s >= 95 else '‚ö†Ô∏è' if sla_5s >= 80 else '‚ùå'}

**Target:** >95% compliance
**Status:** {'Meeting SLA ‚úÖ' if sla_2s >= 95 else 'Below SLA ‚ö†Ô∏è' if sla_2s >= 80 else 'Critical SLA Violation ‚ùå'}
""")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_throughput_analysis(context_data: List[Dict]) -> str:
        """Generate throughput analysis"""
        responses = ["üîÑ **Throughput Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            
            throughput = metrics.get("throughput", 0)
            total_samples = metrics.get("total_samples", 0)
            summary = metrics.get("summary", {})
            duration = summary.get("test_duration_hours", 0)
            
            responses.append(f"""
**üìÅ {filename}**

**Throughput Metrics:**
‚Ä¢ Throughput: {throughput:.1f} requests/second
‚Ä¢ Total Requests: {total_samples:,}
‚Ä¢ Test Duration: {duration:.2f} hours
‚Ä¢ Avg per Hour: {total_samples/duration if duration > 0 else 0:,.0f}

**Analysis:**
""")
            
            if throughput >= 150:
                responses.append("‚úÖ Excellent throughput!")
            elif throughput >= 100:
                responses.append("‚úÖ Good throughput, meets target")
            elif throughput >= 50:
                responses.append("‚ö†Ô∏è Moderate throughput, room for improvement")
            else:
                responses.append("‚ùå Low throughput, scalability concerns")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_availability_analysis(context_data: List[Dict]) -> str:
        """Generate availability analysis"""
        responses = ["üõ°Ô∏è **Availability Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            
            success_rate = summary.get("success_rate", 0)
            error_rate = metrics.get("error_rate", 0) * 100
            
            responses.append(f"""
**üìÅ {filename}**

**Availability Metrics:**
‚Ä¢ Success Rate: {success_rate:.2f}%
‚Ä¢ Failure Rate: {error_rate:.2f}%
‚Ä¢ Uptime: {success_rate:.2f}%

**Target:** 99% availability
**Status:** {'‚úÖ Exceeds target' if success_rate >= 99.5 else '‚úÖ Meets target' if success_rate >= 99 else '‚ö†Ô∏è Below target' if success_rate >= 95 else '‚ùå Critical'}
""")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_comparison(context_data: List[Dict]) -> str:
        """Generate comparison with targets"""
        responses = ["üìä **Performance vs Targets Comparison**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            targets = summary.get("targets", {})
            
            sample_time = metrics.get("sample_time", {})
            avg_time = sample_time.get("mean", 0) / 1000
            p95 = sample_time.get("p95", 0) / 1000
            error_rate = metrics.get("error_rate", 0) * 100
            throughput = metrics.get("throughput", 0)
            success_rate = summary.get("success_rate", 0)
            sla_compliance = summary.get("sla_compliance_2s", 0)
            
            responses.append(f"""
**üìÅ {filename}**

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Availability | {success_rate:.1f}% | {targets.get('availability', 99)}% | {'‚úÖ' if success_rate >= targets.get('availability', 99) else '‚ùå'} |
| Avg Response | {avg_time:.2f}s | <{targets.get('response_time', 2000)/1000:.0f}s | {'‚úÖ' if avg_time < targets.get('response_time', 2000)/1000 else '‚ùå'} |
| Error Rate | {error_rate:.2f}% | <{targets.get('error_rate', 1)}% | {'‚úÖ' if error_rate < targets.get('error_rate', 1) else '‚ùå'} |
| Throughput | {throughput:.0f}/s | {targets.get('throughput', 100)}/s | {'‚úÖ' if throughput >= targets.get('throughput', 100) else '‚ùå'} |
| 95th Percentile | {p95:.2f}s | <{targets.get('p95_percentile', 3000)/1000:.0f}s | {'‚úÖ' if p95 < targets.get('p95_percentile', 3000)/1000 else '‚ùå'} |
| SLA Compliance | {sla_compliance:.1f}% | >{targets.get('sla_compliance', 95)}% | {'‚úÖ' if sla_compliance >= targets.get('sla_compliance', 95) else '‚ùå'} |
""")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_business_impact(context_data: List[Dict]) -> str:
        """Generate business impact analysis"""
        responses = ["üí∞ **Business Impact Assessment**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            
            error_rate = metrics.get("error_rate", 0) * 100
            sample_time = metrics.get("sample_time", {})
            avg_time = sample_time.get("mean", 0) / 1000
            
            responses.append(f"""
**üìÅ {filename}**

**Performance Impact:**
‚Ä¢ Response Time Impact: {'High' if avg_time > 5 else 'Medium' if avg_time > 2 else 'Low'}
‚Ä¢ Error Impact: {'High' if error_rate > 5 else 'Medium' if error_rate > 1 else 'Low'}

**Cost of Current Performance:**
‚Ä¢ Error Rate ({error_rate:.2f}%): {'Major' if error_rate > 5 else 'Moderate'} revenue loss potential
‚Ä¢ Response Time ({avg_time:.1f}s): {'Significant' if avg_time > 5 else 'Moderate'} productivity impact
‚Ä¢ User Experience: {'Poor' if avg_time > 5 or error_rate > 5 else 'Acceptable'}

**Recommended Investment:**
‚Ä¢ Timeline: 6 months for A+ grade
‚Ä¢ Expected ROI: High (first year)
‚Ä¢ Payback Period: 2-4 months
""")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_endpoint_analysis(context_data: List[Dict]) -> str:
        """Generate endpoint performance analysis"""
        responses = ["üîó **Endpoint Performance Analysis**\n"]
        
        for data in context_data:
            filename = data.get("filename", "Unknown")
            metrics = data.get("metrics", {})
            summary = metrics.get("summary", {})
            
            transaction_stats = summary.get("transaction_stats", {})
            request_stats = summary.get("request_stats", {})
            
            responses.append(f"**üìÅ {filename}**\n")
            
            if transaction_stats:
                responses.append("\n**üîù Top 5 Slowest Transactions:**")
                sorted_trans = sorted(transaction_stats.items(), 
                                     key=lambda x: x[1].get('avg_response', 0) or 0, 
                                     reverse=True)[:5]
                for label, data in sorted_trans:
                    avg = data.get('avg_response', 0) / 1000
                    err = data.get('error_rate', 0)
                    responses.append(f"‚Ä¢ {label}: {avg:.2f}s (Error: {err:.2f}%)")
            
            if request_stats:
                responses.append("\n**üîù Top 5 Slowest Requests:**")
                sorted_req = sorted(request_stats.items(), 
                                   key=lambda x: x[1].get('avg_response', 0) or 0, 
                                   reverse=True)[:5]
                for label, data in sorted_req:
                    avg = data.get('avg_response', 0) / 1000
                    err = data.get('error_rate', 0)
                    responses.append(f"‚Ä¢ {label}: {avg:.2f}s (Error: {err:.2f}%)")
        
        return "\n".join(responses)
    
    @staticmethod
    def _generate_specific_metrics(context_data: List[Dict], message: str) -> str:
        """Generate response for specific metric queries"""
        message_lower = message.lower()
        
        if "70" in message_lower or "80" in message_lower or "90" in message_lower:
            return PerformanceChatbot._generate_response_time_analysis(context_data)
        elif "median" in message_lower:
            responses = ["üìä **Median Response Time**\n"]
            for data in context_data:
                filename = data.get("filename", "Unknown")
                metrics = data.get("metrics", {})
                sample_time = metrics.get("sample_time", {})
                median = sample_time.get("median", 0) / 1000
                responses.append(f"**{filename}**: {median:.2f}s")
            return "\n".join(responses)
        else:
            return PerformanceChatbot._generate_general_response(context_data, message)
    
    @staticmethod
    def _generate_general_response(context_data: List[Dict], message: str) -> str:
        """Generate general response"""
        return f"""ü§ñ I analyzed your query: "{message}"

Here's what I found:

""" + PerformanceChatbot._generate_overview_response(context_data) + """

**Need more specific information?** Try asking:
‚Ä¢ "What are the critical issues?"
‚Ä¢ "Show me response time percentiles"
‚Ä¢ "What are your recommendations?"
‚Ä¢ "Compare current vs target metrics"
‚Ä¢ "Show me the slowest endpoints"
"""












